[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Craig Orman, and I am a Junior in the Data Science Program at Iowa State University! When I am not studying or playing with data, I like to read, go hiking, and spend time with my lovely wife Carissa.\nMy current objectives are to start a family, expand my volunteering in my community, and to get accepted into a Statistics PhD program! I love to learn about anything statistics related, but in particular I enjoy financial related data. Housing prices, stock predictions, GDP forecasting, and risk assessments all come to mind. I am excited to not only learn about new models and methods to make predicitions, someday I hope to be able to make my own prediction models that are used in industry."
  },
  {
    "objectID": "classes.html",
    "href": "classes.html",
    "title": "Classes",
    "section": "",
    "text": "A list of classes I feel were particularly interesting!"
  },
  {
    "objectID": "classes.html#certifications",
    "href": "classes.html#certifications",
    "title": "Classes",
    "section": "Certifications",
    "text": "Certifications\n AWS Cloud Practitioner"
  },
  {
    "objectID": "classes.html#data-science",
    "href": "classes.html#data-science",
    "title": "Classes",
    "section": "Data Science",
    "text": "Data Science\n\nData Science 201: Introduction to Data Science\nBasics and introduction to a variety of methods of modeling in Python. Included linear and multi-linear regressions, logistic regression, basic supervised learning with random forests and decision trees, as well as k-means clustering and a brief introduction into unsupervised learning. Model metrics and comparisons, as well as basic statistical concepts were taught."
  },
  {
    "objectID": "classes.html#computer-science",
    "href": "classes.html#computer-science",
    "title": "Classes",
    "section": "Computer Science",
    "text": "Computer Science\n\nComputer Science 227: Introduction to Object Oriented Programming\nIntroduction to the basics of both Java and Object Oriented Programming as a whole. Focus on abstraction, encapsulation, and an early introduction to commonly used algorithms.\n\n\nComputer Science 228: Introduction to Data Structures\nCovers all of the usual data structures in programming, lists, arrayLists, doubly linked lists, trees, binary trees, hashmaps, stacks, queues, and many more! Class taught in Java.\n\n\nComputer Science 311: Introduction to Design and Analysis of Algorithms\nIntroduction to graph algorithms, dyanmic programming, runtime analysis, and proof of correctness for written algorithms. Basic introduction to the concept of P and NP problems. Emphasis on divide and conquer, recursive, and memoization.\n\n\nComputer Science 363: Introduction to Database Management Systems\nIntroduction to SQL style database management, primarily using MySQL. Querying using SQL, and query optimization using different joins. Two phase locking, relational algebra, and other database concepts discussed. Brief introduction to graph databases such as Neo4j.\n\n\nComputer Engineering 419: Software Tools for Large Scale Data Analysis\nIntroduction to Hadoop, distributed databases, and Apache Spark. Looking at issues regarding data durability, and consistency, issues in unstructured and bulk data. Variety of languages and programming abilities used including Pig, Java, Linux systems, and writing parralel code to be run on a cluster.\n\n\nComputer Science 511: Design and Analysis of Algorithms\nGraduate level class that looks at advanced algorithms and handling of P and NP problems. A focus on the Bellman-Ford Method of handling flow networks, conversion between NP Problems and proofs concerning NP completeness, and estimation and heuristic approximations to NP problems."
  },
  {
    "objectID": "classes.html#statistics",
    "href": "classes.html#statistics",
    "title": "Classes",
    "section": "Statistics",
    "text": "Statistics\n\nStatistics 347: Probability and Theory for Data Science\nTheoretical statistics, introduction to a variety of distributions such as binomial, geometric, poisson, normal, gamma, and beta. A variety proofs concerning those distributions in their moment generating functions, expected values and more. Also featured an introduction to point estimators, confidence intervals, and hypothesis testing. Heavy focus on use of R and statistical simulation.\n\n\nStatistics 477: Introduction to categorical variables\nIntroduction to the handling of categorical variables including hypothesis testing, confidence intervals, sampling sizes, tests of independence. Emphasis on visualization as well as numerical summaries. Heavy focus on use of R and statistical simulation.\n\n\nStatistics 588: Statistical Theory for Research Workers\nGraduate level class that gives a more advanced and applied look at distributions with a heavier focus on their applications in research. A variety of new distributions were taught, and an emphasis on learning when and how to model real world data using the distributions in class. R was used for simulations"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Data Analyst Intern\nMay 2023 - August 2023, Bechtel Plant Machinery, Inc., Monroeville, PA\n\nImplemented a custom RoBERTa NLP model to provide a rapid search of business supplier documents hosted on a Streamlit application that allowed continuous user based improvement\nProduced four dynamic Microsoft Power Apps which utilized Power Automate, email triggers, and SharePoint connections and consulted with the UX department for design choices\nDeveloped validation software using rolling averages, peak detection, and curve fitting for physical sensors\n\n\n\n Undergraduate Research Assistant\nAugust 2022 - Present, Center for Statistics and Applications in Forensic Evidence, Ames, IA\n\nRefined previous machine learning models to improve metrics and consistency\nReviewed operating policies to ensure statistics included were rigorous and accurate\nCreated dashboards and web apps to make machine learning more accessible\n\n\n\n Machine Learning Engineering Intern\nMay 2022 - August 2022, Principal Financial Group, Des Moines, IA\n\nDeveloped extensively with AWS Sagemaker, S3, native Sagemaker models, and SKLearn models to train, tune, and test a variety of binary classification models\nPresented and endorsed XGBoost model to business partners throughout the enterprise\nResearched and developed autonomous ML pipelines to ensure that batch predictions were delivered securely and efficiently on a routine basis\n\n\n\n Software Engineering Intern\nMay 2021 - August 2021, Principal Financial Group, Des Moines, IA\n\nDeployed applications and set up automated jobs using Jenkins\nDeveloped a data pipeline on AWS from web apps using Kinesis and Elastic Server\nLed a team to create a Django website for recruiting employees in a 40-hour hackathon\n\n\n\nWeb Developer Intern\nJanuary 2021 - May 2021, eFuneral Solutions, Ames, IA\n\nCreated automated email systems with Elastic Email services in C#\nResolved Jira tickets concerning Stripe, SalesForce, and AWS logging issues\n\n\n\n Security Officer\nJune 2020 - Present, Defense Contracting Activities, Ames IA\n\nSecured a USDA facility ensuring all personnel who entered had proper authorization\nMedical training, CPR and Stop the Bleed certified\n\n\n\n Section Supervisor\nAugust 2019 - May 2020, US Marine Corps, Jacksonville, NC\n\nSupervised, trained, and mentored 14 employees, coordinating different sections and acting as a subject matter expert to higher staff and managers\nSelf-taught online Oracle Applications supply system, ordering over $10,000 worth of parts and handling all modifications and upgrades needed for systems\n\n\n\n Team Leader\nMarch 2017 - August 2019, US Marine Corps, Jacksonville, NC\n\nLed a team of nine employees to complete a variety of objectives in two countries\nLed experimental operations to develop policies and SOP’s for newly introduced technologies\n\n\n\n Radio Operator\nJune 2015 - March 2017, US Marine Corps, Jacksonville, NC\n\nMaintained over 200 secure radio keys and $400,000 worth of classified equipment with 100% accountability.\nHad to develop creative new antennas to overcome rapidly changing terrain conditions.\n\n\n\nAwards\nNavy Achievment Medal for my time as a team leader on deployment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Personal Portfolio and Resume of Craig Orman, Data Science extraordinaire! Do you want to be a part of the future of statistics and machine learning? Then join me in my adventure!\n\nRecent Activities\n\n\nUpcoming Internship\nThis summer (2024) I will be working for John Deere in their finance center in Iowa. I love financial data and am very excited to start my journey with John Deere\n\n\nClasses\nThis semester I am taking Philosophy, Introduction to Machine Learning with the computer science department, Real Analysis I, and my data science capstone class. I hope to gain an intuition into rigorous proofs, the fundamentals of machine learning, and get some more real world experience under my belt in data science.\n\n\nGraduate School\nThis semester I am applying to graduate schools! My objective is to learn more about the internals of machine learning and the surrounding mathematics leading me to someday be able to write new and innovative machine learning and AI algorithms!"
  },
  {
    "objectID": "Models/Linear_Regression.html",
    "href": "Models/Linear_Regression.html",
    "title": "Linear_Regression",
    "section": "",
    "text": "Linear regression is a form of predictive modeling almost everyone has seen in some capacity or another. The general idea is that given a series of data, you assign one of them to be the independent variable (Y) and the dependent variable (X). You try to predict the value of Y given the value of X. Here is an example.\nX = c(3, 5, 7, 9, 11)\nY = c(1, 2, 3, 4, 5)\nplot(X,Y, title(\"Linear Regression Example\"))"
  },
  {
    "objectID": "Models/Linear_Regression.html#equation",
    "href": "Models/Linear_Regression.html#equation",
    "title": "Linear_Regression",
    "section": "Equation",
    "text": "Equation\nThe equation for the typical linear regression is \\(\\bar{Y} = \\beta_0 + \\beta_1x_1 + \\epsilon\\). Where Y is estimated by \\(\\bar{Y}\\), and \\(\\beta_1\\) is the coefficient of x, and \\(\\beta_0\\) is the Y intercept.\nTo find \\(\\beta_1\\) and \\(\\beta_0\\), you can use either gradient descent, linear algebra, or the statistical method. We will focus on the statistical method for now.\n\\[\n\\beta_0 = \\frac{(\\sum y)(\\sum x^2) - (\\sum x)(\\sum xy)}{n(\\sum x^2)-(\\sum x)^2} \\\\\n\\beta_1 = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{n(\\sum x^2) - (\\sum x)^2}\n\\]\nThis will provide the optimal values for the intercept and the slope."
  },
  {
    "objectID": "Models/Linear_Regression.html#assumptions",
    "href": "Models/Linear_Regression.html#assumptions",
    "title": "Linear_Regression",
    "section": "Assumptions",
    "text": "Assumptions\nAll statistical models come with assumptions. It is important to know them, otherwise you risk giving inaccurate results.\n\nThere needs to be a linear relationship between the dependent and independent variables\nIndependence of residuals. If you scatterplot the residuals, there should be no discernible patterns.\nNormal distribution of residuals. When you put the residuals in a histogram, it should form a normal distribution centered on 0, or close to.\nHomoscedasticity, or common variance. This commonly known as the megaphone effect. When plotting the values, they should not grow in variability as the independent value increases."
  },
  {
    "objectID": "Models/Linear_Regression.html#evaluation",
    "href": "Models/Linear_Regression.html#evaluation",
    "title": "Linear_Regression",
    "section": "Evaluation",
    "text": "Evaluation\nSo how ‘good’ is our linear regression? This can be a tough question to answer in many cases. For linear regression there’s a few commonly sourced methods, but the \\(R^2\\) method is the most prevalent, although many would dispute whether or not it should be.\n\\[\nR^2 = 1 - \\frac{\\sum (y_i-b_0-b_1x_i)^2}{\\sum (y_i-\\bar{y_i})^2}\n\\] The \\(R^2\\) is measured between 0 and 1, with 1 being a perfect match and 0 being a complete mismatch. The \\(R^2\\) explains the amount of variation that is captured by the developed model."
  },
  {
    "objectID": "Models/Linear_Regression.html#example",
    "href": "Models/Linear_Regression.html#example",
    "title": "Linear_Regression",
    "section": "Example",
    "text": "Example\nIn R, doing a linear regression is very simple. Many people have done this same technique in other programming languages, and even in excel and other tools.\nIn this example, we will be predicting weight based on height. Some sample data from Tutorialspoint.com is provided below.\n\nheight &lt;- c(151, 174, 138, 186, 128, 136, 179, 163, 152, 131)\nweight &lt;- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)\n\nplot(weight, height, col=\"blue\", main = \"Height and Weight Data\",\nabline(lm(weight~height)), cex = 1.3, pch = 16, xlab = \"Weight in Kg\", ylab = \"Height in cm\")\n\n\n\nrelation &lt;- lm(weight~height)\n\n\\[\nweight = -38.4551 + height * 0.6746\n\\] From a quick look at our plot we have no reason to contest the linear relationship. Now lets check our residuals.\n\nexpected = -38.4551 + height * 0.6746\ndf = data.frame(height, weight, expected)\ndf$residual = expected - weight\nprint(df)\n\n   height weight expected residual\n1     151     63  63.4095   0.4095\n2     174     81  78.9253  -2.0747\n3     138     56  54.6397  -1.3603\n4     186     91  87.0205  -3.9795\n5     128     47  47.8937   0.8937\n6     136     57  53.2905  -3.7095\n7     179     76  82.2983   6.2983\n8     163     72  71.5047  -0.4953\n9     152     62  64.0841   2.0841\n10    131     48  49.9175   1.9175\n\n\n\nplot(df$weight, df$residual, abline(h=0), xlab = \"Weight\", ylab = \"Residual\", main = \"Independence of Residuals\")\n\n\n\n\nThere does not appear to be any particular patterns in the residuals.\n\nhist(df$residual, xlab = \"Residuals\", main = \"Histogram of Residuals\")\n\n\n\n\nIn this case, our output should create some caution. The sample size here is too small to create a convincing normal distribution, although you can see the mean is close to 0. This would indicate that you should in fact seek more more data, a new model, or to use this model only under careful watch.\nAnd finally for homoscedasticity, we look to the original plot of the data and see that there is no major ‘megaphoning’ of the data, but again with this small of a dataset, it can be very hard to tell.\nNow that we have determined this model is accurate enough for our needs, barring our one questionable assumption, we will test its \\(R^2\\). This is best done in R using the built in packages. Below we can see the \\(R^2\\) is 0.9548 which is pretty good! You can also see a variety of other metrics around our data below.\n\nsummary(relation)\n\n\nCall:\nlm(formula = weight ~ height)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3002 -1.6629  0.0412  1.8944  3.9775 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -38.45509    8.04901  -4.778  0.00139 ** \nheight        0.67461    0.05191  12.997 1.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.253 on 8 degrees of freedom\nMultiple R-squared:  0.9548,    Adjusted R-squared:  0.9491 \nF-statistic: 168.9 on 1 and 8 DF,  p-value: 1.164e-06"
  },
  {
    "objectID": "Models/Linear_Regression.html#bibliography",
    "href": "Models/Linear_Regression.html#bibliography",
    "title": "Linear_Regression",
    "section": "Bibliography",
    "text": "Bibliography\n\nhttps://www.analyticsvidhya.com/blog/2021/10/everything-you-need-to-know-about-linear-regression/\nhttps://www.tutorialspoint.com/r/r_linear_regression.htm"
  },
  {
    "objectID": "Models.html",
    "href": "Models.html",
    "title": "Models",
    "section": "",
    "text": "This is a running list of models and Data Science techniques that I have practiced and learned. I hope to provide not only examples and code of the techniques, but also the math behind them to allow a better understanding.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLinear_Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "myProjects/401k-rollover.html",
    "href": "myProjects/401k-rollover.html",
    "title": "PFG 401k Rollover Project",
    "section": "",
    "text": "During my 2022 Summer internship with Principal Financial Group, I was assigned a data science project along with William Edwards to try and predict if Principal customers would be willing to do a roll-in of their non-local retirement savings.\nAll data and products were stored on the Principal servers, so unfortunately I can’t share too much about it. The project involved a variety of factors including numerical and categorical. Extensive data cleaning was required, much of it involved contacting other parties and verifying what was a reasonable boundary for a particular feature, and then excluding any rows in the data that were outside of the boundaries.\nWe fit approximate ten different models and compared them all to see which gave us the best AUC and F1score. The model was a prototype, so more specific hyper parameter tuning was left to be decided by the particular client it went to. If the mass-email team got a version, they would want higher recall, or in other words, its better to send more emails and bother people than it is to send less emails and miss potential customers. However if the model was given to the call center, the opposite would be true, you would tune for percision, or in other words its better to miss out on potential clients, as long as the ones we are calling are sure bets. Our base model attempted to find the happy medium using the F1score.\nThe model used several hundred thousand rows of data and about 80 columns worth features. We used an XGBoost model on AWS Sagemaker."
  },
  {
    "objectID": "myProjects/nat-gas.html",
    "href": "myProjects/nat-gas.html",
    "title": "Natural Gas Price Predicition",
    "section": "",
    "text": "For my DS 201 final project in 2021, I did an ANOVA time series prediction on the price of natural gas in the US using Python and Jupyter Notebooks. Data is collected from the U.S. Energy information Administration\nThe price of Natural Gas is highly seasonal, and is very dependent on political and environmental effects.There is significant variability even over the course of a year, and between 2000 and 2010, there was a significant change in the baseline price. In about 2008 there was a significant outlier in the price of natural gas. Since 2008, the variability of the price of natural gas has increased, but the mean price from an initial guess is fairly constant. Below is a time chart of the price of natural gas, data points were monthly.\n\n\n\nNatural Gas Prices\n\n\nThe first and most obvious idea I had to try and capture seasonality was to track monthly change in price. The price of natural gas goes up, and is more volatile in the summer months.\n\n\n\nNatural Gas Monthly Variability\n\n\nNow to try a new modelling method I had never attempted before, I tried to fit an ANOVA model, this required me to check if the data was stationary, and if it had significant correlation. In the chart below, if the lines go outside the blue shaded zone, it fails the correlation test. You can see that in this data, the data didn’t pass very comfortably, which means I would need to do some transformations of the data to ensure the ANOVA fit goes correctly. For the purposes of the class, I decided to press anyways. In the future, significant data manipulation would be required.\n\n\n\nNatural Gas Monthly Variability\n\n\nTo fit the model, I used all the data up until 2021-02 for training, and all the other data as test data. I used the statsmodels.tsa.holtwinters package to fit the ANOVA model, and it produced the following chart that shows the breakdown of our data. The ANOVA equation is simply put (Price = Trend + Seasonal + Residual).\n\n\n\nNatural Gas Monthly Variability\n\n\nI compared the modeling predictions against the test data, and we can see that the residual model is quite close to the actual price. The \\(R^2\\) value is 0.925. So our model did surprisingly well considering we only used time data to predict it! There are many variables related to the price, and we didn’t consider most of them.\n\n\n\nNatural Gas Test Predictions\n\n\nFinally, we make a prediction of the (at the time) future!\n\n\n\nNatural Gas Future Predictions"
  },
  {
    "objectID": "my_projects.html",
    "href": "my_projects.html",
    "title": "Projects",
    "section": "",
    "text": "I have worked on several significant data science projects involving data analysis, cleaning, and modelling, here are a few examples!\n\n\n\n\n\n\n\n\n  \n\n\n\n\nNatural Gas Price Predicition\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPFG 401k Rollover Project\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "While an Undergraduate Research Assistant at the Center for Statistical Analysis of Forensic Evidence, I worked for Dr. Heike Hofmann with my teammate Naga Vempati to make an application in R that evaluates the quality of a given bullet scan to see if it will be adequate for striation comparison to another bullet. The intention of said comparison is to see if the bullets can reasonably be determined to have been fired from the same barrel.\nMany of the images such as the one below suffer from a variety of problems, namely holes and feathering. The app is a random forest model that uses seven features to predict the quality of the a scan. The particular problem of the image is also assesed. Five random forests, one for each tracked kind of problem, and lack thereof, each doing a binary comparison to detect if its particular problem is present. The max problem score is selected as the particular problem.\n\n\n\nBullet Striation with Missing Sections"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Publications",
    "section": "",
    "text": "While an Undergraduate Research Assistant at the Center for Statistical Analysis of Forensic Evidence, I worked for Dr. Heike Hofmann with my teammate Naga Vempati to make an application in R that evaluates the quality of a given bullet scan to see if it will be adequate for striation comparison to another bullet. The intention of said comparison is to see if the bullets can reasonably be determined to have been fired from the same barrel.\nMany of the images such as the one below suffer from a variety of problems, namely holes and feathering. The app is a random forest model that uses seven features to predict the quality of the a scan. The particular problem of the image is also assesed. Five random forests, one for each tracked kind of problem, and lack thereof, each doing a binary comparison to detect if its particular problem is present. The max problem score is selected as the particular problem.\n\n\n\nBullet Striation with Missing Sections"
  }
]